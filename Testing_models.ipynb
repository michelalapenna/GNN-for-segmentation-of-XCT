{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import*\n",
    "from utils import*\n",
    "from models import*\n",
    "from Train_Eval_Test import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# If you use GPU, the device should be cuda\n",
    "print('Device: {}'.format(device))\n",
    "\n",
    "# Hyperparameters of the model\n",
    "\n",
    "args = {\n",
    "    'device': device,\n",
    "    'heads': 2,\n",
    "    'num_features' : 1,\n",
    "    'hidden' : 100, # 100 for simple GNN, 50 for ViG model\n",
    "    'num_classes' : 6,\n",
    "    'dropout': 0.001,\n",
    "    'alpha' : 0.1,\n",
    "    'lr': 0.001,\n",
    "    'epochs': 200,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side = 512 # The side of the original volume\n",
    "new_side = 64 # The side of the sub-volume on which we construct the graph\n",
    "stride = 28 # The stride we use in extracting the overlapping sub-volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import synthetic test volume and extract subvolumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = raw_to_tensor(\"CVSynth.raw\", side)\n",
    "\n",
    "labels_test = raw_to_tensor(\"CVSynth_Labels.raw\", side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.tensor(view_as_windows(features_test.numpy(), (new_side,new_side,new_side), step=stride).reshape(-1,new_side**3,1))\n",
    "\n",
    "y_test = torch.tensor(view_as_windows(labels_test.numpy(), (new_side,new_side,new_side), step=stride).reshape(-1,new_side**3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the features of the four experimental volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimental volumes conditioned with a Non-Local Means filter\n",
    "\n",
    "features_1_exp = raw_to_tensor(\"CV1_NLM8.raw\", side)\n",
    "features_2_exp = raw_to_tensor(\"CV2_NLM8.raw\", side)\n",
    "features_3_exp = raw_to_tensor(\"CV3_NLM8.raw\", side)\n",
    "features_4_exp = raw_to_tensor(\"CV4_NLM8.raw\", side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimental volumes conditioned with BAM SynthCOND\n",
    "\n",
    "features_1_exp = raw_to_tensor(\"CV1_AI.raw\", side)\n",
    "features_2_exp = raw_to_tensor(\"CV2_AI.raw\", side)\n",
    "features_3_exp = raw_to_tensor(\"CV3_AI.raw\", side)\n",
    "features_4_exp = raw_to_tensor(\"CV4_AI.raw\", side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we extract the subvolumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1_exp = torch.tensor(view_as_windows(features_1_exp.numpy(), (new_side,new_side,new_side), step=stride).reshape(-1,new_side**3,1))\n",
    "x_2_exp = torch.tensor(view_as_windows(features_2_exp.numpy(), (new_side,new_side,new_side), step=stride).reshape(-1,new_side**3,1))\n",
    "x_3_exp = torch.tensor(view_as_windows(features_3_exp.numpy(), (new_side,new_side,new_side), step=stride).reshape(-1,new_side**3,1))\n",
    "x_4_exp = torch.tensor(view_as_windows(features_4_exp.numpy(), (new_side,new_side,new_side), step=stride).reshape(-1,new_side**3,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we import the four manually-labelled slices, one for each volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_1_exp = tif_to_tensor(\"CV1 Labels - Slice 339.tif\", side)\n",
    "labels_2_exp = tif_to_tensor(\"CV2 Labels - Slice 139.tif\", side)\n",
    "labels_3_exp = tif_to_tensor(\"CV3 Labels - Slice 219.tif\", side)\n",
    "labels_4_exp = tif_to_tensor(\"CV4 Labels - Slice 059.tif\", side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a graph from each subvolume by connecting each voxel to its nearest 6 neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_neigh = 6\n",
    "\n",
    "cloud = torch.cartesian_prod(torch.tensor(range(0, new_side)), torch.tensor(range(0, new_side)), torch.tensor(range(0, new_side)))\n",
    "edges = create_edges(k_neigh, cloud)\n",
    "adj = SparseTensor(row=edges[0], col=edges[1], sparse_sizes=(new_side**3,new_side**3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataset of subvolumes from the synthetic volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list_test = []\n",
    "\n",
    "for i in range(x_test.size()[0]):\n",
    "    data_list_test.append(Data(x=x_test[i], edge_index=edges, y=y_test[i]))\n",
    "\n",
    "test_loader = DataLoader(data_list_test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataset of subvolumes from every experimental volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list_test_1 = []\n",
    "\n",
    "for i in range(x_1_exp.size()[0]):\n",
    "    data_list_test_1.append(Data(x=x_1_exp[i], edge_index=edges, y=labels_1_exp))\n",
    "\n",
    "test_loader_1 = DataLoader(data_list_test_1, batch_size=1, shuffle=False)\n",
    "\n",
    "data_list_test_2 = []\n",
    "\n",
    "for i in range(x_2_exp.size()[0]):\n",
    "    data_list_test_2.append(Data(x=x_2_exp[i], edge_index=edges, y=labels_2_exp))\n",
    "\n",
    "test_loader_2 = DataLoader(data_list_test_2, batch_size=1, shuffle=False)\n",
    "\n",
    "data_list_test_3 = []\n",
    "\n",
    "for i in range(x_3_exp.size()[0]):\n",
    "    data_list_test_3.append(Data(x=x_3_exp[i], edge_index=edges, y=labels_3_exp))\n",
    "\n",
    "test_loader_3 = DataLoader(data_list_test_3, batch_size=1, shuffle=False)\n",
    "\n",
    "data_list_test_4 = []\n",
    "\n",
    "for i in range(x_4_exp.size()[0]):\n",
    "    data_list_test_4.append(Data(x=x_4_exp[i], edge_index=edges, y=labels_4_exp))\n",
    "\n",
    "test_loader_4 = DataLoader(data_list_test_4, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch size is set to 1 when testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the four manually-labelled slices to compare them with the model's segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(labels_1_exp, vmin=0, vmax=5)\n",
    "plt.savefig(\"Labels_1\")\n",
    "plt.close()\n",
    "\n",
    "plt.imshow(labels_2_exp, vmin=0, vmax=5)\n",
    "plt.savefig(\"Labels_2\")\n",
    "plt.close()\n",
    "\n",
    "plt.imshow(labels_3_exp, vmin=0, vmax=5)\n",
    "plt.savefig(\"Labels_3\")\n",
    "plt.close()\n",
    "\n",
    "plt.imshow(labels_4_exp, vmin=0, vmax=5)\n",
    "plt.savefig(\"Labels_4\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test each trained GNN model (10 in total) on the subvolumes extracted from the four experimental volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each experimental volume, we reconstruct 6 probability volumes (one for each class) and we assign to each voxel the highest probability class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the Dice score only for the manually-labelled slices and we write it on an external file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The number of Steps to reconstruct the original volume from the evaluation on the subvolumes:\n",
    "\n",
    "steps = int((side - new_side) / stride + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    \n",
    "  model = ViG(batch, new_side, args['num_features'], args['hidden'], args['num_classes'], args['dropout']).to(device)\n",
    "  model.load_state_dict(torch.load('ViG_' + str(i) + '.h5'))\n",
    "  model.eval()\n",
    "\n",
    "  eval_obj = Test(model, device, args['classes'])\n",
    "\n",
    "  # synthetic\n",
    "\n",
    "  preds = extract_overlap_pred(eval_obj, test_loader, args['num_classes'], steps, new_side, side, stride)\n",
    "\n",
    "  dice = dice(preds, labels_test, average='none', num_classes=6)\n",
    "  dice_overall = dice(preds, labels_test, average='macro')\n",
    "  data = [i, 100 * dice[1].numpy(), 100 * dice[2].numpy(), 100 * dice[3].numpy(), 100 * dice[4].numpy(), 100 * dice[5].numpy(),\n",
    "            100 * dice_overall.numpy()]\n",
    "\n",
    "  preds_1 = extract_overlap_pred(eval_obj, test_loader_1, args['num_classes'], steps, new_side, side, stride)\n",
    "\n",
    "  plt.imshow(preds_1[338], vmin=0, vmax=5) # the manually-labelled slice for the first volume is number 339\n",
    "  plt.savefig(\"Preds_1_\" + str(i))\n",
    "  plt.close()\n",
    "\n",
    "  dice_1 = dice(preds_1[338], labels_1_exp, average='none', num_classes=6)\n",
    "  dice_1_overall = dice(preds_1[338], labels_1_exp, average='macro')\n",
    "  data_1 = [i, 100 * dice_1[1].numpy(), 100 * dice_1[2].numpy(), 100 * dice_1[3].numpy(), 100 * dice_1[4].numpy(), 100 * dice_1[5].numpy(),\n",
    "            100 * dice_1_overall.numpy()]\n",
    "\n",
    "  preds_2 = extract_overlap_pred(eval_obj, test_loader_2)\n",
    "\n",
    "  labels_2_exp[labels_2_exp == 6] = 0 # label remaining voids as Alluminium matrix\n",
    "\n",
    "  plt.imshow(preds_2[138], vmin=0, vmax=5) # the manually-labelled slice for the second volume is number 139\n",
    "  plt.savefig(\"Preds_2_\" + str(i))\n",
    "  plt.close()\n",
    "\n",
    "  dice_2 = dice(preds_2[138], labels_2_exp, average='none', num_classes=6)\n",
    "  dice_2_overall = dice(preds_2[138], labels_2_exp, average='macro')\n",
    "  data_2 = [i, 100 * dice_2[1].numpy(), 100 * dice_2[2].numpy(), 100 * dice_2[3].numpy(), 100 * dice_2[4].numpy(), 100 * dice_2[5].numpy(),\n",
    "            100 * dice_2_overall.numpy()]\n",
    "\n",
    "  preds_3 = extract_overlap_pred(eval_obj, test_loader_3)\n",
    "\n",
    "  labels_3_exp[labels_3_exp == 6] = 0 # label remaining voids as Alluminium matrix\n",
    "\n",
    "  plt.imshow(preds_3[218], vmin=0, vmax=5) # the manually-labelled slice for the third volume is number 219\n",
    "  plt.savefig(\"Preds_3\" + str(i))\n",
    "  plt.close()\n",
    "\n",
    "  dice_3 = dice(preds_3[218], labels_3_exp, average='none', num_classes=6)\n",
    "  dice_3_overall = dice(preds_3[218], labels_3_exp, average='macro')\n",
    "  data_3 = [i, 100 * dice_3[1].numpy(), 100 * dice_3[2].numpy(), 100 * dice_3[3].numpy(), 100 * dice_3[4].numpy(), 100 * dice_3[5].numpy(),\n",
    "            100 * dice_3_overall.numpy()]\n",
    "\n",
    "  preds_4 = extract_overlap_pred(eval_obj, test_loader_4)\n",
    "\n",
    "  labels_4_exp[labels_4_exp == 6] = 0 # label remaining voids as Alluminium matrix\n",
    "\n",
    "  plt.imshow(preds_4[58], vmin=0, vmax=5) # the manually-labelled slice for the fourth volume is number 59\n",
    "  plt.savefig(\"Preds_4\" + str(i))\n",
    "  plt.close()\n",
    "\n",
    "  dice_4 = dice(preds_4[58], labels_4_exp, average='none', num_classes=6)\n",
    "  dice_4_overall = dice(preds_4[58], labels_4_exp, average='macro')\n",
    "  data_4 = [i, 100 * dice_4[1].numpy(), 100 * dice_4[2].numpy(), 100 * dice_4[3].numpy(), 100 * dice_4[4].numpy(), 100 * dice_4[5].numpy(),\n",
    "            100 * dice_4_overall.numpy()]\n",
    "\n",
    "  with open('Test_ViG.csv', 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(data)\n",
    "        writer.writerow(data_1)\n",
    "        writer.writerow(data_2)\n",
    "        writer.writerow(data_3)\n",
    "        writer.writerow(data_4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
